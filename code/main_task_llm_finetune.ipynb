{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## finetuning\n",
    "#### huggingfaceä¸Šã«ã‚ã‚‹LLMã‚’installã—ã¦ãã¦finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jubuntu/anaconda3/envs/unsloth/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "338c7de6a6374cda9771658e4ad41d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e0c68719c642f39aaef46a1946de27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/54 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'å·¦è‚ºä¸Šè‘‰ã«é•·å¾„ 12cm ã®è…«ç˜¤ã‚’èªã‚ã¾ã™ã€‚å¼“éƒ¨å¤§å‹•è„ˆã‚„è‚ºå‹•è„ˆæµ¸æ½¤ãŒç–‘ã‚ã‚Œã¾ã™ã€‚\\nT4 ã¨è€ƒãˆã¾ã™ã€‚\\nå·¦è‚ºé–€ã‚„ç¸¦éš”ãƒªãƒ³ãƒ‘ç¯€#6 ã®è…«å¤§ãŒè¦‹ã‚‰ã‚Œã¾ã™ã€‚è»¢ç§»ãŒç–‘ã‚ã‚Œã¾ã™ã€‚N2 ã¨è€ƒãˆã¾ã™ã€‚\\nèƒ¸æ°´ã¯èªã‚ã¾ã›ã‚“ã€‚\\nå·¦ç¬¬ 3 è‚‹éª¨ã¯è…«ç˜¤ã«ã‚ˆã‚‹éª¨ç ´å£ŠãŒè¦‹ã‚‰ã‚Œã¾ã™ã€‚ä»–æ’®åƒç¯„å›²ã®éª¨ã«é éš”è»¢ç§»ã‚’ç–‘ã†æ‰€è¦‹ã¯æŒ‡æ‘˜ã§ãã¾ã›ã‚“ã€‚\\næ’®åƒç¯„å›²ã®è…¹éƒ¨ã«æœ‰æ„æ‰€è¦‹ã¯æŒ‡æ‘˜ã§ãã¾ã›ã‚“ã€‚', 'completion': 'T4 N2 M0', 'input_ids': [128000, 30591, 112568, 15682, 104622, 106241, 26854, 103242, 104599, 38641, 1811, 88852, 16144, 83125, 20230, 75146, 109669, 50834, 57942, 118, 23706, 234, 114556, 39926, 40053, 20230, 37656, 102800, 122225, 29295, 125545, 1811, 116748, 27479, 17620, 104770, 15682, 88852, 16144, 30271, 44, 30537, 66115, 41401, 20230, 107253, 26955, 254, 127145, 1811, 99849, 32977, 117952, 101860, 20230, 88852, 30512, 112854, 58942, 5486, 58318, 58942, 109115, 83125, 55031, 114652, 78698, 107290, 30271, 44, 17620, 104770, 30512, 102647, 104127, 72315, 19066, 51, 15, 197, 53229, 102404, 101921, 104, 114431, 235, 30512, 104736, 62004, 100604, 198, 51, 285, 197, 17905, 105871, 32943, 100204, 107106, 104328, 100604, 29295, 25827, 10110, 7063, 18595, 7942, 304, 10109, 123407, 57942, 118, 102212, 25287, 112987, 71289, 29295, 28713, 16144, 126513, 5486, 105843, 103350, 13153, 17620, 67669, 15, 6358, 32149, 59739, 103429, 104721, 37087, 33014, 67669, 111486, 18, 6358, 198, 51, 16, 197, 101921, 104, 114431, 235, 16144, 105843, 103350, 13153, 17620, 67669, 111486, 18, 6358, 5486, 57942, 118, 125845, 26274, 241, 110366, 111140, 115716, 108422, 30297, 27929, 47884, 50834, 107106, 17129, 47884, 20230, 117633, 78183, 115032, 5486, 105065, 95221, 36651, 46456, 101200, 50834, 32149, 25827, 15024, 105755, 16325, 20119, 95, 111363, 27699, 116, 103558, 97, 29295, 95221, 36651, 46456, 117785, 17905, 104736, 62004, 102204, 100604, 10110, 17663, 26854, 78183, 43514, 36668, 95221, 36651, 46456, 20230, 82317, 104127, 16995, 100604, 23954, 51, 16, 8318, 23249, 50034, 83747, 27699, 116, 103558, 97, 34171, 101921, 118, 102802, 109954, 15024, 25827, 100204, 116186, 25827, 72342, 16995, 72342, 25827, 108911, 25827, 5232, 105494, 105843, 103350, 25287, 30512, 20379, 15024, 5486, 105843, 103350, 13153, 17620, 67669, 111486, 15, 13, 20, 6358, 32149, 59739, 103429, 104721, 37087, 33014, 67669, 111486, 18, 6358, 720, 51, 16, 64, 197, 105843, 103350, 13153, 17620, 67669, 111486, 16, 6358, 16556, 32149, 59739, 51, 285, 9458, 51, 16, 8318, 102052, 120019, 21105, 103, 16995, 198, 51, 16, 65, 197, 105843, 103350, 13153, 17620, 67669, 29, 16, 6358, 16556, 32149, 59739, 111486, 17, 6358, 198, 51, 16, 66, 197, 105843, 103350, 13153, 17620, 67669, 29, 17, 6358, 16556, 32149, 59739, 111486, 18, 6358, 198, 51, 17, 197, 105843, 103350, 13153, 17620, 67669, 29, 18, 6358, 16556, 32149, 59739, 111486, 20, 6358, 5486, 125845, 105843, 103350, 13153, 17620, 67669, 111486, 18, 6358, 72661, 88852, 16144, 16995, 101860, 33121, 32149, 103195, 102944, 198, 23249, 23249, 36668, 95221, 36651, 46456, 20230, 82317, 104369, 29295, 95221, 36651, 17620, 58699, 238, 34048, 102052, 82317, 81219, 100604, 198, 23249, 23249, 26274, 241, 110366, 111140, 115716, 20230, 27699, 116, 103558, 97, 198, 23249, 23249, 57942, 118, 103722, 103296, 89046, 106307, 54926, 105494, 9554, 125845, 15120, 110366, 37087, 33014, 16144, 43568, 95221, 57942, 118, 32149, 119962, 59657, 60239, 57942, 118, 114052, 107407, 198, 51, 17, 64, 197, 105843, 103350, 13153, 17620, 67669, 29, 18, 6358, 16556, 32149, 59739, 111486, 19, 6358, 198, 51, 17, 65, 197, 105843, 103350, 13153, 17620, 67669, 29, 19, 6358, 16556, 32149, 59739, 111486, 20, 6358, 198, 51, 18, 197, 105843, 103350, 13153, 17620, 67669, 29, 20, 6358, 16556, 32149, 59739, 111486, 22, 6358, 5486, 125845, 105843, 103350, 13153, 17620, 67669, 111486, 20, 6358, 72661, 88852, 16144, 16995, 101860, 33121, 32149, 103195, 102944, 198, 23249, 23249, 110426, 110366, 111140, 115716, 5486, 111140, 110426, 10110, 9712, 2521, 26858, 57440, 36254, 30512, 96412, 104004, 65459, 109203, 113849, 101365, 106613, 5486, 64209, 115716, 16144, 16995, 101860, 33121, 111201, 109467, 27699, 116, 103558, 97, 198, 23249, 23249, 42016, 15120, 105065, 123727, 16937, 89046, 106307, 26854, 106966, 101921, 104, 114431, 235, 102971, 107792, 198, 51, 19, 197, 105843, 103350, 13153, 17620, 67669, 83354, 22, 6358, 5486, 125845, 119448, 30813, 30512, 99397, 124059, 109203, 113849, 115716, 5486, 102057, 99, 113849, 100204, 116186, 30297, 32149, 47884, 5486, 64209, 26274, 241, 110745, 104473, 36651, 36785, 16995, 76622, 42891, 32149, 25827, 5486, 95221, 36651, 50834, 32149, 25827, 5486, 95543, 18904, 101365, 106613, 15682, 25827, 121867, 15024, 25827, 76622, 16995, 5486, 102456, 45893, 124304, 47884, 104405, 5486, 103433, 236, 33014, 59739, 102334, 16995, 5486, 95221, 36651, 17620, 58699, 238, 34048, 50834, 32149, 25827, 104369, 25827, 50834, 104369, 111363, 27699, 116, 103558, 97, 5486, 101182, 16995, 15682, 42016, 110366, 16144, 108901, 26854, 100472, 57942, 118, 105065, 32943, 112987, 101200, 100604, 16144, 106966, 101921, 104, 114431, 235, 102971, 107792, 1432, 45, 15, 197, 125151, 107805, 80805, 107792, 108436, 60634, 110214, 198, 45, 16, 197, 29295, 25827, 16144, 101182, 57942, 118, 19732, 112361, 110366, 16144, 95221, 36651, 46456, 41642, 116310, 32149, 59739, 14, 125845, 112361, 110366, 16144, 57942, 118, 103722, 5486, 57942, 118, 123727, 107805, 80805, 107792, 111363, 108436, 60634, 107407, 198, 45, 17, 197, 29295, 25827, 16144, 101182, 57942, 118, 19732, 112361, 110366, 16144, 102057, 99, 113849, 32149, 59739, 14, 125845, 95221, 36651, 17620, 58699, 238, 34048, 105755, 17297, 16144, 107805, 80805, 107792, 111363, 108436, 60634, 107407, 198, 45, 18, 197, 29295, 25827, 16144, 101182, 57942, 118, 19732, 95543, 103359, 110366, 16144, 102057, 99, 113849, 5486, 57942, 118, 103722, 112987, 32977, 25827, 5486, 112361, 110366, 101182, 16995, 15682, 95543, 103359, 110366, 16144, 25580, 7741, 250, 64936, 119049, 112371, 25827, 116826, 32149, 47884, 50834, 25827, 10110, 61075, 16144, 119049, 107079, 65459, 109349, 244, 110135, 17905, 91739, 102, 30813, 22957, 59739, 100204, 107106, 32149, 10110, 109349, 244, 110135, 120938, 16144, 47884, 109416, 64121, 120732, 107805, 80805, 107792, 111363, 108436, 60634, 107407, 1432, 44, 15, 197, 110596, 113849, 108436, 60634, 110214, 198, 44, 16, 197, 110596, 113849, 108436, 60634, 107407, 198, 44, 16, 64, 197, 29295, 25827, 16144, 101182, 57942, 118, 19732, 95543, 103359, 110366, 16144, 57942, 118, 123727, 102971, 107792, 5486, 111140, 115716, 125845, 64209, 115716, 16144, 102971, 107792, 5486, 109161, 34171, 111140, 53610, 30591, 47884, 72342, 16995, 50834, 107106, 17663, 16995, 10110, 29295, 25827, 16144, 101182, 57942, 118, 19732, 112361, 110366, 5486, 95543, 103359, 110366, 65459, 109161, 34171, 64209, 115833, 95, 53610, 30591, 47884, 72342, 16995, 15024, 25827, 16144, 30297, 17663, 16995, 29295, 64121, 110071, 198, 44, 16, 65, 197, 57942, 118, 116749, 112028, 26274, 241, 32648, 16995, 43514, 108422, 30297, 50834, 111363, 110904, 102404, 110596, 113849, 108436, 60634, 107407, 198, 44, 16, 66, 197, 57942, 118, 116749, 112028, 26274, 241, 32648, 125845, 43240, 26274, 241, 32648, 28713, 108422, 30297, 50834, 111363, 43240, 102404, 110596, 113849, 108436, 60634, 107407, 1038, 88852, 16144, 83125, 30512, 98499, 104127, 5486, 30271, 44, 17620, 104770, 30512, 37656, 105184, 20230, 121507, 15024, 5486, 59614, 101860, 88852, 16144, 115707, 16556, 20834, 48634, 39926, 72315, 29411, 51, 41082, 31868, 13099, 47217, 60, 452, 41082, 31868, 13099, 47217, 60, 386, 41082, 31868, 13099, 47217, 2595, 78659, 57942, 118, 17905, 105065, 20230, 101544, 67669, 220, 717, 6358, 97718, 101921, 104, 114431, 97, 30512, 104736, 62004, 33541, 1811, 13870, 241, 34048, 27384, 91875, 101171, 230, 71289, 57942, 118, 91875, 101171, 230, 27699, 116, 103558, 97, 29295, 107750, 123671, 33541, 9174, 51, 19, 112031, 113727, 33541, 9174, 78659, 57942, 118, 103722, 71289, 102057, 99, 113849, 107805, 80805, 107792, 2, 21, 97718, 101921, 104, 27384, 29295, 91774, 102204, 33541, 1811, 108436, 60634, 29295, 107750, 123671, 33541, 1811, 45, 17, 112031, 113727, 33541, 9174, 111140, 53610, 15682, 104736, 62004, 107515, 9174, 78659, 30537, 220, 18, 107399, 233, 110135, 15682, 101921, 104, 114431, 97, 111848, 110135, 105150, 127258, 29295, 91774, 102204, 33541, 107751, 125152, 66201, 119318, 116310, 16144, 110135, 20230, 110596, 113849, 108436, 60634, 30512, 107750, 30297, 32938, 91774, 15682, 64467, 112419, 103082, 107515, 9174, 125152, 66201, 119318, 116310, 16144, 113351, 34048, 20230, 19361, 37689, 32938, 91774, 15682, 64467, 112419, 103082, 107515, 1811, 105961, 48634, 5232], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [128000, 51, 19, 452, 17, 386, 15], 'original_inputs': 'ã‚ãªãŸã¯å„ªç§€ãªåŒ»å¸«ã§ã™ã€‚ä»¥ä¸‹ã®æ–‡ç« ã«åŸºã¥ãè‚ºç™Œã«é–¢ã—ã¦å¸¸ã«æ­£ã—ã„åˆ¤æ–­ãŒã§ãã¾ã™ã€‚é€²è¡Œåº¦åˆ†é¡ã¯ä»¥ä¸‹ã®TNMç¬¬ï¼˜ç‰ˆã«æº–æ‹ ã—ã¦ã„ã¾ã™ã€‚ä½•ã‚‚è¨€ã‚ãšã«ä»¥ä¸‹ã‚’è¦šãˆã€ä¸ãˆã‚‰ã‚ŒãŸæ–‡ç« ã‹ã‚‰ã‚ˆãè€ƒãˆã¦TNMåˆ†é¡ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚\\n\\n\\n\\nT0\\tåŸç™ºè…«ç˜ã‚’èªã‚ãªã„\\nTis\\tä¸Šçš®å†…ã˜ã‚‡ã†ã²ãªã„ãŒã‚“ï¼ˆcarcinoma in situï¼‰ï¼šè‚ºé‡å‹ã¯ã„ã‚„ãŒãŸã®å ´åˆã¯ã€å……å®Ÿæˆåˆ†å¾„0cmã‹ã¤ç—…å¤‰å…¨ä½“å¾„â‰¦3cm\\nT1\\tè…«ç˜ã®å……å®Ÿæˆåˆ†å¾„â‰¦3cmã€è‚ºã¾ãŸã¯è‡“å´èƒ¸è†œãã†ãããã‚‡ã†ã¾ãã«è¦†ã‚ã‚Œã¦ã„ã‚‹ã€è‘‰æ°—ç®¡æ”¯ã‚ˆã†ãã‹ã‚“ã—ã‚ˆã‚Šä¸­æ¢ã¸ã®æµ¸æ½¤ãŒæ°—ç®¡æ”¯é¡ä¸Šèªã‚ã‚‰ã‚Œãªã„ï¼ˆã™ãªã‚ã¡ä¸»æ°—ç®¡æ”¯ã«åŠã‚“ã§ã„ãªã„ï¼‰\\nT1mi\\u3000å¾®å°‘æµ¸æ½¤æ€§è…ºã³ã—ã‚‡ã†ã—ã‚“ã˜ã‚…ã‚“ã›ã„ã›ã‚“ ãŒã‚“ï¼šéƒ¨åˆ†å……å®Ÿå‹ã‚’ç¤ºã—ã€å……å®Ÿæˆåˆ†å¾„â‰¦0.5cmã‹ã¤ç—…å¤‰å…¨ä½“å¾„â‰¦3cm \\nT1a\\tå……å®Ÿæˆåˆ†å¾„â‰¦1cmã§ã‹ã¤Tisãƒ»T1miã«ã¯ç›¸å½“ã—ãªã„\\nT1b\\tå……å®Ÿæˆåˆ†å¾„>1cmã§ã‹ã¤â‰¦2cm\\nT1c\\tå……å®Ÿæˆåˆ†å¾„>2cmã§ã‹ã¤â‰¦3cm\\nT2\\tå……å®Ÿæˆåˆ†å¾„>3cmã§ã‹ã¤â‰¦5cmã€ã¾ãŸã¯å……å®Ÿæˆåˆ†å¾„â‰¦3cmã§ã‚‚ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã§ã‚ã‚‹ã‚‚ã®\\n\\u3000\\u3000ä¸»æ°—ç®¡æ”¯ã«åŠã¶ãŒæ°—ç®¡åˆ†å²éƒ¨ã«ã¯åŠã°ãªã„\\n\\u3000\\u3000è‡“å´èƒ¸è†œã«æµ¸æ½¤\\n\\u3000\\u3000è‚ºé–€ã¾ã§é€£ç¶šã™ã‚‹éƒ¨åˆ†çš„ã¾ãŸã¯ä¸€å´å…¨ä½“ã®ç„¡æ°—è‚ºã‹é–‰å¡æ€§è‚ºç‚ãŒã‚ã‚‹\\nT2a\\tå……å®Ÿæˆåˆ†å¾„>3cmã§ã‹ã¤â‰¦4cm\\nT2b\\tå……å®Ÿæˆåˆ†å¾„>4cmã§ã‹ã¤â‰¦5cm\\nT3\\tå……å®Ÿæˆåˆ†å¾„>5cmã§ã‹ã¤â‰¦7cmã€ã¾ãŸã¯å……å®Ÿæˆåˆ†å¾„â‰¦5cmã§ã‚‚ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã§ã‚ã‚‹ã‚‚ã®\\n\\u3000\\u3000å£å´èƒ¸è†œã€èƒ¸å£ï¼ˆsuperior sulcus tumorã‚’å«ã‚€ï¼‰ã€æ¨ªéš”ç¥çµŒã€å¿ƒè†œã®ã„ãšã‚Œã‹ã«ç›´æ¥æµ¸æ½¤\\n\\u3000\\u3000åŒä¸€è‘‰å†…ã®ä¸é€£ç¶šãªå‰¯è…«ç˜çµç¯€\\nT4\\tå……å®Ÿæˆåˆ†å¾„ï¼7cmã€ã¾ãŸã¯å¤§ãã•ã‚’å•ã‚ãšæ¨ªéš”è†œã€ç¸¦éš”ã˜ã‚…ã†ã‹ãã€å¿ƒè‡“ã€å¤§è¡€ç®¡ã ã„ã‘ã£ã‹ã‚“ã€æ°—ç®¡ãã‹ã‚“ã€åå›ç¥çµŒã¯ã‚“ã‹ã„ã—ã‚“ã‘ã„ã€é£Ÿé“ã—ã‚‡ãã©ã†ã€æ¤ä½“ã¤ã„ãŸã„ã€æ°—ç®¡åˆ†å²éƒ¨ãã‹ã‚“ã¶ã‚“ãã¶ã¸ã®æµ¸æ½¤ã€ã‚ã‚‹ã„ã¯åŒå´ã®ç•°ãªã£ãŸè‚ºè‘‰å†…ã¯ã„ã‚ˆã†ãªã„ã®å‰¯è…«ç˜çµç¯€\\n\\n\\nN0\\tæ‰€å±ãƒªãƒ³ãƒ‘ç¯€è»¢ç§»ãªã—\\nN1\\tãŒã‚“ã®ã‚ã‚‹è‚ºã¨åŒã˜å´ã®æ°—ç®¡æ”¯å‘¨å›²ã‹ã¤/ã¾ãŸã¯åŒã˜å´ã®è‚ºé–€ã€è‚ºå†…ã®ãƒªãƒ³ãƒ‘ç¯€ã¸ã®è»¢ç§»ãŒã‚ã‚‹\\nN2\\tãŒã‚“ã®ã‚ã‚‹è‚ºã¨åŒã˜å´ã®ç¸¦éš”ã‹ã¤/ã¾ãŸã¯æ°—ç®¡åˆ†å²éƒ¨ã‚ˆã‚Šä¸‹ã®ãƒªãƒ³ãƒ‘ç¯€ã¸ã®è»¢ç§»ãŒã‚ã‚‹\\nN3\\tãŒã‚“ã®ã‚ã‚‹è‚ºã¨åå¯¾å´ã®ç¸¦éš”ã€è‚ºé–€ã¯ã„ã‚‚ã‚“ã€åŒã˜å´ã‚ã‚‹ã„ã¯åå¯¾å´ã®å‰æ–œè§’ç­‹ãœã‚“ã—ã‚ƒã‹ããã‚“ï¼ˆé¦–ã®ç­‹è‚‰ï¼‰ã€é–éª¨ä¸Šçª©ã•ã“ã¤ã˜ã‚‡ã†ã‹ï¼ˆé–éª¨ã®ä¸Šã®ãã¼ã¿ï¼‰ã®ãƒªãƒ³ãƒ‘ç¯€ã¸ã®è»¢ç§»ãŒã‚ã‚‹\\n\\n\\nM0\\té éš”è»¢ç§»ãªã—\\nM1\\té éš”è»¢ç§»ãŒã‚ã‚‹\\nM1a\\tãŒã‚“ã®ã‚ã‚‹è‚ºã¨åå¯¾å´ã®è‚ºå†…ã®çµç¯€ã€èƒ¸è†œã¾ãŸã¯å¿ƒè†œã®çµç¯€ã€æ‚ªæ€§èƒ¸æ°´ã‚ãã›ã„ãã‚‡ã†ã™ã„ï¼ˆãŒã‚“ã®ã‚ã‚‹è‚ºã¨åŒã˜å´ã€åå¯¾å´ï¼‰ã€æ‚ªæ€§å¿ƒåš¢æ°´ã‚ãã›ã„ã—ã‚“ã®ã†ã™ã„ãŒã¿ã‚‰ã‚Œã‚‹\\nM1b\\tè‚ºä»¥å¤–ã®ä¸€è‡“å™¨ã„ã¡ãã†ãã¸ã®å˜ç™ºé éš”è»¢ç§»ãŒã‚ã‚‹\\nM1c\\tè‚ºä»¥å¤–ã®ä¸€è‡“å™¨ã¾ãŸã¯å¤šè‡“å™¨ãŸãã†ãã¸ã®å¤šç™ºé éš”è»¢ç§»ãŒã‚ã‚‹\\n\\n\\n\\nä»¥ä¸‹ã®æ–‡ç« ã‚’èª­ã‚“ã§ã€TNMåˆ†é¡ã‚’æ­£ç¢ºã«é¸æŠã—ã€å¿…ãšä»¥ä¸‹ã®å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š\\nT<number>[optional_letter] N<number>[optional_letter] M<number>[optional_letter]\\n\\nå·¦è‚ºä¸Šè‘‰ã«é•·å¾„ 12cm ã®è…«ç˜¤ã‚’èªã‚ã¾ã™ã€‚å¼“éƒ¨å¤§å‹•è„ˆã‚„è‚ºå‹•è„ˆæµ¸æ½¤ãŒç–‘ã‚ã‚Œã¾ã™ã€‚\\nT4 ã¨è€ƒãˆã¾ã™ã€‚\\nå·¦è‚ºé–€ã‚„ç¸¦éš”ãƒªãƒ³ãƒ‘ç¯€#6 ã®è…«å¤§ãŒè¦‹ã‚‰ã‚Œã¾ã™ã€‚è»¢ç§»ãŒç–‘ã‚ã‚Œã¾ã™ã€‚N2 ã¨è€ƒãˆã¾ã™ã€‚\\nèƒ¸æ°´ã¯èªã‚ã¾ã›ã‚“ã€‚\\nå·¦ç¬¬ 3 è‚‹éª¨ã¯è…«ç˜¤ã«ã‚ˆã‚‹éª¨ç ´å£ŠãŒè¦‹ã‚‰ã‚Œã¾ã™ã€‚ä»–æ’®åƒç¯„å›²ã®éª¨ã«é éš”è»¢ç§»ã‚’ç–‘ã†æ‰€è¦‹ã¯æŒ‡æ‘˜ã§ãã¾ã›ã‚“ã€‚\\næ’®åƒç¯„å›²ã®è…¹éƒ¨ã«æœ‰æ„æ‰€è¦‹ã¯æŒ‡æ‘˜ã§ãã¾ã›ã‚“ã€‚ å‡ºåŠ›ï¼š', 'original_targets': 'T4 N2 M0', 'text': 'ã‚ãªãŸã¯å„ªç§€ãªåŒ»å¸«ã§ã™ã€‚ä»¥ä¸‹ã®æ–‡ç« ã«åŸºã¥ãè‚ºç™Œã«é–¢ã—ã¦å¸¸ã«æ­£ã—ã„åˆ¤æ–­ãŒã§ãã¾ã™ã€‚é€²è¡Œåº¦åˆ†é¡ã¯ä»¥ä¸‹ã®TNMç¬¬ï¼˜ç‰ˆã«æº–æ‹ ã—ã¦ã„ã¾ã™ã€‚ä½•ã‚‚è¨€ã‚ãšã«ä»¥ä¸‹ã‚’è¦šãˆã€ä¸ãˆã‚‰ã‚ŒãŸæ–‡ç« ã‹ã‚‰ã‚ˆãè€ƒãˆã¦TNMåˆ†é¡ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚\\n\\n\\n\\nT0\\tåŸç™ºè…«ç˜ã‚’èªã‚ãªã„\\nTis\\tä¸Šçš®å†…ã˜ã‚‡ã†ã²ãªã„ãŒã‚“ï¼ˆcarcinoma in situï¼‰ï¼šè‚ºé‡å‹ã¯ã„ã‚„ãŒãŸã®å ´åˆã¯ã€å……å®Ÿæˆåˆ†å¾„0cmã‹ã¤ç—…å¤‰å…¨ä½“å¾„â‰¦3cm\\nT1\\tè…«ç˜ã®å……å®Ÿæˆåˆ†å¾„â‰¦3cmã€è‚ºã¾ãŸã¯è‡“å´èƒ¸è†œãã†ãããã‚‡ã†ã¾ãã«è¦†ã‚ã‚Œã¦ã„ã‚‹ã€è‘‰æ°—ç®¡æ”¯ã‚ˆã†ãã‹ã‚“ã—ã‚ˆã‚Šä¸­æ¢ã¸ã®æµ¸æ½¤ãŒæ°—ç®¡æ”¯é¡ä¸Šèªã‚ã‚‰ã‚Œãªã„ï¼ˆã™ãªã‚ã¡ä¸»æ°—ç®¡æ”¯ã«åŠã‚“ã§ã„ãªã„ï¼‰\\nT1mi\\u3000å¾®å°‘æµ¸æ½¤æ€§è…ºã³ã—ã‚‡ã†ã—ã‚“ã˜ã‚…ã‚“ã›ã„ã›ã‚“ ãŒã‚“ï¼šéƒ¨åˆ†å……å®Ÿå‹ã‚’ç¤ºã—ã€å……å®Ÿæˆåˆ†å¾„â‰¦0.5cmã‹ã¤ç—…å¤‰å…¨ä½“å¾„â‰¦3cm \\nT1a\\tå……å®Ÿæˆåˆ†å¾„â‰¦1cmã§ã‹ã¤Tisãƒ»T1miã«ã¯ç›¸å½“ã—ãªã„\\nT1b\\tå……å®Ÿæˆåˆ†å¾„>1cmã§ã‹ã¤â‰¦2cm\\nT1c\\tå……å®Ÿæˆåˆ†å¾„>2cmã§ã‹ã¤â‰¦3cm\\nT2\\tå……å®Ÿæˆåˆ†å¾„>3cmã§ã‹ã¤â‰¦5cmã€ã¾ãŸã¯å……å®Ÿæˆåˆ†å¾„â‰¦3cmã§ã‚‚ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã§ã‚ã‚‹ã‚‚ã®\\n\\u3000\\u3000ä¸»æ°—ç®¡æ”¯ã«åŠã¶ãŒæ°—ç®¡åˆ†å²éƒ¨ã«ã¯åŠã°ãªã„\\n\\u3000\\u3000è‡“å´èƒ¸è†œã«æµ¸æ½¤\\n\\u3000\\u3000è‚ºé–€ã¾ã§é€£ç¶šã™ã‚‹éƒ¨åˆ†çš„ã¾ãŸã¯ä¸€å´å…¨ä½“ã®ç„¡æ°—è‚ºã‹é–‰å¡æ€§è‚ºç‚ãŒã‚ã‚‹\\nT2a\\tå……å®Ÿæˆåˆ†å¾„>3cmã§ã‹ã¤â‰¦4cm\\nT2b\\tå……å®Ÿæˆåˆ†å¾„>4cmã§ã‹ã¤â‰¦5cm\\nT3\\tå……å®Ÿæˆåˆ†å¾„>5cmã§ã‹ã¤â‰¦7cmã€ã¾ãŸã¯å……å®Ÿæˆåˆ†å¾„â‰¦5cmã§ã‚‚ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã§ã‚ã‚‹ã‚‚ã®\\n\\u3000\\u3000å£å´èƒ¸è†œã€èƒ¸å£ï¼ˆsuperior sulcus tumorã‚’å«ã‚€ï¼‰ã€æ¨ªéš”ç¥çµŒã€å¿ƒè†œã®ã„ãšã‚Œã‹ã«ç›´æ¥æµ¸æ½¤\\n\\u3000\\u3000åŒä¸€è‘‰å†…ã®ä¸é€£ç¶šãªå‰¯è…«ç˜çµç¯€\\nT4\\tå……å®Ÿæˆåˆ†å¾„ï¼7cmã€ã¾ãŸã¯å¤§ãã•ã‚’å•ã‚ãšæ¨ªéš”è†œã€ç¸¦éš”ã˜ã‚…ã†ã‹ãã€å¿ƒè‡“ã€å¤§è¡€ç®¡ã ã„ã‘ã£ã‹ã‚“ã€æ°—ç®¡ãã‹ã‚“ã€åå›ç¥çµŒã¯ã‚“ã‹ã„ã—ã‚“ã‘ã„ã€é£Ÿé“ã—ã‚‡ãã©ã†ã€æ¤ä½“ã¤ã„ãŸã„ã€æ°—ç®¡åˆ†å²éƒ¨ãã‹ã‚“ã¶ã‚“ãã¶ã¸ã®æµ¸æ½¤ã€ã‚ã‚‹ã„ã¯åŒå´ã®ç•°ãªã£ãŸè‚ºè‘‰å†…ã¯ã„ã‚ˆã†ãªã„ã®å‰¯è…«ç˜çµç¯€\\n\\n\\nN0\\tæ‰€å±ãƒªãƒ³ãƒ‘ç¯€è»¢ç§»ãªã—\\nN1\\tãŒã‚“ã®ã‚ã‚‹è‚ºã¨åŒã˜å´ã®æ°—ç®¡æ”¯å‘¨å›²ã‹ã¤/ã¾ãŸã¯åŒã˜å´ã®è‚ºé–€ã€è‚ºå†…ã®ãƒªãƒ³ãƒ‘ç¯€ã¸ã®è»¢ç§»ãŒã‚ã‚‹\\nN2\\tãŒã‚“ã®ã‚ã‚‹è‚ºã¨åŒã˜å´ã®ç¸¦éš”ã‹ã¤/ã¾ãŸã¯æ°—ç®¡åˆ†å²éƒ¨ã‚ˆã‚Šä¸‹ã®ãƒªãƒ³ãƒ‘ç¯€ã¸ã®è»¢ç§»ãŒã‚ã‚‹\\nN3\\tãŒã‚“ã®ã‚ã‚‹è‚ºã¨åå¯¾å´ã®ç¸¦éš”ã€è‚ºé–€ã¯ã„ã‚‚ã‚“ã€åŒã˜å´ã‚ã‚‹ã„ã¯åå¯¾å´ã®å‰æ–œè§’ç­‹ãœã‚“ã—ã‚ƒã‹ããã‚“ï¼ˆé¦–ã®ç­‹è‚‰ï¼‰ã€é–éª¨ä¸Šçª©ã•ã“ã¤ã˜ã‚‡ã†ã‹ï¼ˆé–éª¨ã®ä¸Šã®ãã¼ã¿ï¼‰ã®ãƒªãƒ³ãƒ‘ç¯€ã¸ã®è»¢ç§»ãŒã‚ã‚‹\\n\\n\\nM0\\té éš”è»¢ç§»ãªã—\\nM1\\té éš”è»¢ç§»ãŒã‚ã‚‹\\nM1a\\tãŒã‚“ã®ã‚ã‚‹è‚ºã¨åå¯¾å´ã®è‚ºå†…ã®çµç¯€ã€èƒ¸è†œã¾ãŸã¯å¿ƒè†œã®çµç¯€ã€æ‚ªæ€§èƒ¸æ°´ã‚ãã›ã„ãã‚‡ã†ã™ã„ï¼ˆãŒã‚“ã®ã‚ã‚‹è‚ºã¨åŒã˜å´ã€åå¯¾å´ï¼‰ã€æ‚ªæ€§å¿ƒåš¢æ°´ã‚ãã›ã„ã—ã‚“ã®ã†ã™ã„ãŒã¿ã‚‰ã‚Œã‚‹\\nM1b\\tè‚ºä»¥å¤–ã®ä¸€è‡“å™¨ã„ã¡ãã†ãã¸ã®å˜ç™ºé éš”è»¢ç§»ãŒã‚ã‚‹\\nM1c\\tè‚ºä»¥å¤–ã®ä¸€è‡“å™¨ã¾ãŸã¯å¤šè‡“å™¨ãŸãã†ãã¸ã®å¤šç™ºé éš”è»¢ç§»ãŒã‚ã‚‹\\n\\n\\n\\nä»¥ä¸‹ã®æ–‡ç« ã‚’èª­ã‚“ã§ã€TNMåˆ†é¡ã‚’æ­£ç¢ºã«é¸æŠã—ã€å¿…ãšä»¥ä¸‹ã®å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š\\nT<number>[optional_letter] N<number>[optional_letter] M<number>[optional_letter]\\n\\nå·¦è‚ºä¸Šè‘‰ã«é•·å¾„ 12cm ã®è…«ç˜¤ã‚’èªã‚ã¾ã™ã€‚å¼“éƒ¨å¤§å‹•è„ˆã‚„è‚ºå‹•è„ˆæµ¸æ½¤ãŒç–‘ã‚ã‚Œã¾ã™ã€‚\\nT4 ã¨è€ƒãˆã¾ã™ã€‚\\nå·¦è‚ºé–€ã‚„ç¸¦éš”ãƒªãƒ³ãƒ‘ç¯€#6 ã®è…«å¤§ãŒè¦‹ã‚‰ã‚Œã¾ã™ã€‚è»¢ç§»ãŒç–‘ã‚ã‚Œã¾ã™ã€‚N2 ã¨è€ƒãˆã¾ã™ã€‚\\nèƒ¸æ°´ã¯èªã‚ã¾ã›ã‚“ã€‚\\nå·¦ç¬¬ 3 è‚‹éª¨ã¯è…«ç˜¤ã«ã‚ˆã‚‹éª¨ç ´å£ŠãŒè¦‹ã‚‰ã‚Œã¾ã™ã€‚ä»–æ’®åƒç¯„å›²ã®éª¨ã«é éš”è»¢ç§»ã‚’ç–‘ã†æ‰€è¦‹ã¯æŒ‡æ‘˜ã§ãã¾ã›ã‚“ã€‚\\næ’®åƒç¯„å›²ã®è…¹éƒ¨ã«æœ‰æ„æ‰€è¦‹ã¯æŒ‡æ‘˜ã§ãã¾ã›ã‚“ã€‚ å‡ºåŠ›ï¼š'}\n"
     ]
    }
   ],
   "source": [
    "## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from accelerate import Accelerator\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# GPUã®ç¢ºèª\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰\n",
    "model_name = \"pfnet/Llama3-Preferred-MedSwallow-70B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def prepare_dataset(jsonl_file_path):\n",
    "    data = []\n",
    "    with open(jsonl_file_path, 'r', encoding='utf-8') as jsonl_file:\n",
    "        for line in jsonl_file:\n",
    "            entry = json.loads(line.strip())\n",
    "            data.append({\n",
    "                \"prompt\": entry[\"prompt\"],\n",
    "                \"completion\": entry[\"completion\"]\n",
    "            })\n",
    "    return Dataset.from_list(data)\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ­ãƒ¼ãƒ‰\n",
    "jsonl_file_path_train = \"../finetune_jsons/finetune_dataset_train.jsonl\"\n",
    "dataset_train = prepare_dataset(jsonl_file_path_train)\n",
    "jsonl_file_path_val = \"../finetune_jsons/finetune_dataset_val.jsonl\"\n",
    "dataset_val = prepare_dataset(jsonl_file_path_val)\n",
    "\n",
    "with open('../tnm_prompt.txt', 'r', encoding='utf-8') as file:\n",
    "    tnm_prompt_text = file.read()\n",
    "\n",
    "tnm_prompt_text_base = (\n",
    "    \"ã‚ãªãŸã¯å„ªç§€ãªåŒ»å¸«ã§ã™ã€‚ä»¥ä¸‹ã®æ–‡ç« ã«åŸºã¥ãè‚ºç™Œã«é–¢ã—ã¦å¸¸ã«æ­£ã—ã„åˆ¤æ–­ãŒã§ãã¾ã™ã€‚\"\n",
    "    \"é€²è¡Œåº¦åˆ†é¡ã¯ä»¥ä¸‹ã®TNMç¬¬ï¼˜ç‰ˆã«æº–æ‹ ã—ã¦ã„ã¾ã™ã€‚ä½•ã‚‚è¨€ã‚ãšã«ä»¥ä¸‹ã‚’è¦šãˆã€ä¸ãˆã‚‰ã‚ŒãŸæ–‡ç« ã‹ã‚‰ã‚ˆãè€ƒãˆã¦TNMåˆ†é¡ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚\\n\\n\"\n",
    ")\n",
    "tnm_prompt = (\n",
    "    f\"{tnm_prompt_text_base}\\n\\n\"\n",
    "    f\"{tnm_prompt_text}\\n\\n\"\n",
    "    \"ä»¥ä¸‹ã®æ–‡ç« ã‚’èª­ã‚“ã§ã€TNMåˆ†é¡ã‚’æ­£ç¢ºã«é¸æŠã—ã€å¿…ãšä»¥ä¸‹ã®å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š\\n\"\n",
    "    \"T<number>[optional_letter] N<number>[optional_letter] M<number>[optional_letter]\\n\\n\"\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    \n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [f\"{tnm_prompt}{prompt} å‡ºåŠ›ï¼š\" for prompt in examples['prompt']]\n",
    "    targets = [completion for completion in examples['completion']]\n",
    "    \n",
    "    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º (å…¥åŠ›)\n",
    "    model_inputs = tokenizer(inputs, truncation=True, padding=False)\n",
    "    \n",
    "    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º (ãƒ©ãƒ™ãƒ«)\n",
    "    labels = tokenizer(targets, truncation=True, padding=False)\n",
    "    \n",
    "    # -100 ã‚’ãƒ©ãƒ™ãƒ«ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã«è¨­å®š\n",
    "    labels[\"input_ids\"] = [\n",
    "        [-100 if token == tokenizer.pad_token_id else token for token in label]\n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    # å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚‚ä¿æŒ\n",
    "    model_inputs[\"original_inputs\"] = inputs\n",
    "    model_inputs[\"original_targets\"] = targets\n",
    "    model_inputs[\"text\"] = inputs\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿æŒã—ã¤ã¤ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆ\n",
    "train_dataset = dataset_train.map(preprocess_function, batched=True)\n",
    "eval_dataset = dataset_val.map(preprocess_function, batched=True)\n",
    "\n",
    "\n",
    "\n",
    "##sample output\n",
    "print(train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## inferenceã®ã‚³ãƒ¼ãƒ‰\n",
    "\n",
    "# æ¨è«–é–¢æ•°ã®å®šç¾©\n",
    "def inference(dataset, model, tokenizer, max_length=2048):\n",
    "    results = []\n",
    "    for data in tqdm(dataset):\n",
    "        # å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—\n",
    "        input_text = data[\"original_inputs\"]\n",
    "        \n",
    "        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º (ãƒ¢ãƒ‡ãƒ«ãŒç†è§£ã§ãã‚‹å½¢å¼ã«å¤‰æ›)\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=max_length).to(model.device)\n",
    "        \n",
    "        # ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                max_length=max_length,  # æœ€å¤§å‡ºåŠ›é•·\n",
    "                early_stopping=True,  # æ—©æœŸçµ‚äº†ã‚’æœ‰åŠ¹åŒ–\n",
    "                eos_token_id=tokenizer.eos_token_id,  # EOSãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®š\n",
    "                do_sample=True,  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’æœ‰åŠ¹åŒ–\n",
    "                top_k=50,  # ä¸Šä½kãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰é¸æŠ\n",
    "                top_p=0.95  # ãƒˆãƒ¼ã‚¯ãƒ³ã®ç¢ºç‡è³ªé‡\n",
    "            )\n",
    "\n",
    "        \n",
    "        # ãƒ‡ã‚³ãƒ¼ãƒ‰ (ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›)\n",
    "        output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # çµæœã‚’ä¿å­˜\n",
    "        results.append({\n",
    "            \"input\": input_text,\n",
    "            \"generated_output\": output_text,\n",
    "            \"expected_output\": data[\"original_targets\"]\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: BNB_CUDA_VERSION=121 environment variable detected; loading libbitsandbytes_cuda121.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2024.11.10: Fast Llama patching. Transformers:4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.425 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1. CUDA: 8.6. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b56a9ecc68234fe5ad305a75575c501b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0475159fae4696b1a34b4f5ba4f7ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pfnet/Llama3-Preferred-MedSwallow-70B does not have a padding token! Will use pad_token = <|reserved_special_token_250|>.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel \n",
    "from unsloth import is_bfloat16_supported\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from datasets import load_dataset\n",
    "max_seq_length = 2048 # Supports RoPE Scaling interally, so choose any!\n",
    "# Get LAION dataset\n",
    "# url = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\n",
    "# dataset = load_dataset(\"json\", data_files = {\"train\" : url}, split = \"train\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## finetuneå‰ã®ç¢ºèª\n",
    "\n",
    "# æ¨è«–å®Ÿè¡Œ\n",
    "inference_results = inference(eval_dataset[:3], model, tokenizer)\n",
    "\n",
    "# çµæœã‚’è¡¨ç¤º\n",
    "for i, result in enumerate(inference_results):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"Input: {result['input']}\")\n",
    "    print(f\"Generated Output: {result['generated_output']}\")\n",
    "    print(f\"Expected Output: {result['expected_output']}\")\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.11.10 patched 80 layers with 80 QKV layers, 80 O layers and 80 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    label_pad_token_id=-100,\n",
    "    padding=True,  # å‹•çš„ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’æœ‰åŠ¹ã«ã™ã‚‹\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = data_collator,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part=\"ã‚ãªãŸã¯å„ªç§€ãªåŒ»å¸«ã§ã™ã€‚ä»¥ä¸‹ã®æ–‡ç« ã«åŸºã¥ãè‚ºç™Œã«é–¢ã—ã¦å¸¸ã«æ­£ã—ã„åˆ¤æ–­ãŒã§ãã¾ã™ã€‚\",  # ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³éƒ¨åˆ†ã‚’æŒ‡å®š\n",
    "    response_part=\"å‡ºåŠ›ï¼š\"  # ãƒ¬ã‚¹ãƒãƒ³ã‚¹éƒ¨åˆ†ã‚’æŒ‡å®š\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 108 | Num Epochs = 5\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 60\n",
      " \"-____-\"     Number of trainable parameters = 207,093,760\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdac6631b3e0416fbf989ad17e9ee4b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8381, 'grad_norm': 0.4072749614715576, 'learning_rate': 4e-05, 'epoch': 0.07}\n",
      "{'loss': 1.7384, 'grad_norm': 0.36444002389907837, 'learning_rate': 8e-05, 'epoch': 0.15}\n",
      "{'loss': 1.7724, 'grad_norm': 0.3491908013820648, 'learning_rate': 0.00012, 'epoch': 0.22}\n",
      "{'loss': 1.7671, 'grad_norm': 0.5786504745483398, 'learning_rate': 0.00016, 'epoch': 0.3}\n",
      "{'loss': 1.6427, 'grad_norm': 0.4702806770801544, 'learning_rate': 0.0002, 'epoch': 0.37}\n",
      "{'loss': 1.5077, 'grad_norm': 0.48127204179763794, 'learning_rate': 0.00019636363636363636, 'epoch': 0.44}\n",
      "{'loss': 1.3497, 'grad_norm': 0.6133865714073181, 'learning_rate': 0.00019272727272727274, 'epoch': 0.52}\n",
      "{'loss': 1.349, 'grad_norm': 3.6691014766693115, 'learning_rate': 0.0001890909090909091, 'epoch': 0.59}\n",
      "{'loss': 1.1671, 'grad_norm': 0.5333468914031982, 'learning_rate': 0.00018545454545454545, 'epoch': 0.67}\n",
      "{'loss': 1.1274, 'grad_norm': 0.4787834882736206, 'learning_rate': 0.00018181818181818183, 'epoch': 0.74}\n",
      "{'loss': 1.1379, 'grad_norm': 0.4744940996170044, 'learning_rate': 0.0001781818181818182, 'epoch': 0.81}\n",
      "{'loss': 1.0749, 'grad_norm': 0.43891769647598267, 'learning_rate': 0.00017454545454545454, 'epoch': 0.89}\n",
      "{'loss': 1.0666, 'grad_norm': 0.45089587569236755, 'learning_rate': 0.0001709090909090909, 'epoch': 0.96}\n",
      "{'loss': 1.5115, 'grad_norm': 0.7233561873435974, 'learning_rate': 0.00016727272727272728, 'epoch': 1.04}\n",
      "{'loss': 0.9219, 'grad_norm': 0.38212257623672485, 'learning_rate': 0.00016363636363636366, 'epoch': 1.11}\n",
      "{'loss': 0.9122, 'grad_norm': 0.4429636597633362, 'learning_rate': 0.00016, 'epoch': 1.19}\n",
      "{'loss': 0.7687, 'grad_norm': 0.37760111689567566, 'learning_rate': 0.00015636363636363637, 'epoch': 1.26}\n",
      "{'loss': 0.9016, 'grad_norm': 0.38100793957710266, 'learning_rate': 0.00015272727272727275, 'epoch': 1.33}\n",
      "{'loss': 1.042, 'grad_norm': 0.4578867554664612, 'learning_rate': 0.0001490909090909091, 'epoch': 1.41}\n",
      "{'loss': 0.6612, 'grad_norm': 0.37774598598480225, 'learning_rate': 0.00014545454545454546, 'epoch': 1.48}\n",
      "{'loss': 0.9471, 'grad_norm': 0.44463348388671875, 'learning_rate': 0.00014181818181818184, 'epoch': 1.56}\n",
      "{'loss': 0.8164, 'grad_norm': 0.4159707725048065, 'learning_rate': 0.0001381818181818182, 'epoch': 1.63}\n",
      "{'loss': 0.941, 'grad_norm': 0.4307209849357605, 'learning_rate': 0.00013454545454545455, 'epoch': 1.7}\n",
      "{'loss': 0.7133, 'grad_norm': 0.38697025179862976, 'learning_rate': 0.00013090909090909093, 'epoch': 1.78}\n",
      "{'loss': 0.89, 'grad_norm': 0.40672358870506287, 'learning_rate': 0.00012727272727272728, 'epoch': 1.85}\n",
      "{'loss': 0.8776, 'grad_norm': 0.40502941608428955, 'learning_rate': 0.00012363636363636364, 'epoch': 1.93}\n",
      "{'loss': 1.2088, 'grad_norm': 0.5576397776603699, 'learning_rate': 0.00012, 'epoch': 2.0}\n",
      "{'loss': 0.6984, 'grad_norm': 0.4023154079914093, 'learning_rate': 0.00011636363636363636, 'epoch': 2.07}\n",
      "{'loss': 0.8098, 'grad_norm': 0.3994697332382202, 'learning_rate': 0.00011272727272727272, 'epoch': 2.15}\n",
      "{'loss': 0.6024, 'grad_norm': 0.39395031332969666, 'learning_rate': 0.00010909090909090909, 'epoch': 2.22}\n",
      "{'loss': 0.6845, 'grad_norm': 0.44504621624946594, 'learning_rate': 0.00010545454545454545, 'epoch': 2.3}\n",
      "{'loss': 0.6537, 'grad_norm': 0.4104446768760681, 'learning_rate': 0.00010181818181818181, 'epoch': 2.37}\n",
      "{'loss': 0.6738, 'grad_norm': 0.45404812693595886, 'learning_rate': 9.818181818181818e-05, 'epoch': 2.44}\n",
      "{'loss': 0.6134, 'grad_norm': 0.4455665946006775, 'learning_rate': 9.454545454545455e-05, 'epoch': 2.52}\n",
      "{'loss': 0.5681, 'grad_norm': 0.5403136014938354, 'learning_rate': 9.090909090909092e-05, 'epoch': 2.59}\n",
      "{'loss': 0.6857, 'grad_norm': 0.5693321228027344, 'learning_rate': 8.727272727272727e-05, 'epoch': 2.67}\n",
      "{'loss': 0.6993, 'grad_norm': 0.5823256969451904, 'learning_rate': 8.363636363636364e-05, 'epoch': 2.74}\n",
      "{'loss': 0.6455, 'grad_norm': 0.6148393154144287, 'learning_rate': 8e-05, 'epoch': 2.81}\n",
      "{'loss': 0.5755, 'grad_norm': 0.49833372235298157, 'learning_rate': 7.636363636363637e-05, 'epoch': 2.89}\n",
      "{'loss': 0.6437, 'grad_norm': 0.47686371207237244, 'learning_rate': 7.272727272727273e-05, 'epoch': 2.96}\n",
      "{'loss': 0.8716, 'grad_norm': 0.7296509146690369, 'learning_rate': 6.90909090909091e-05, 'epoch': 3.04}\n",
      "{'loss': 0.517, 'grad_norm': 0.4170914888381958, 'learning_rate': 6.545454545454546e-05, 'epoch': 3.11}\n",
      "{'loss': 0.6783, 'grad_norm': 0.5008776187896729, 'learning_rate': 6.181818181818182e-05, 'epoch': 3.19}\n",
      "{'loss': 0.4893, 'grad_norm': 0.5498774647712708, 'learning_rate': 5.818181818181818e-05, 'epoch': 3.26}\n",
      "{'loss': 0.4206, 'grad_norm': 0.46303197741508484, 'learning_rate': 5.4545454545454546e-05, 'epoch': 3.33}\n",
      "{'loss': 0.4863, 'grad_norm': 0.49719223380088806, 'learning_rate': 5.090909090909091e-05, 'epoch': 3.41}\n",
      "{'loss': 0.5056, 'grad_norm': 0.6029546856880188, 'learning_rate': 4.7272727272727275e-05, 'epoch': 3.48}\n",
      "{'loss': 0.389, 'grad_norm': 0.5620816946029663, 'learning_rate': 4.3636363636363636e-05, 'epoch': 3.56}\n",
      "{'loss': 0.5438, 'grad_norm': 0.8492675423622131, 'learning_rate': 4e-05, 'epoch': 3.63}\n",
      "{'loss': 0.5325, 'grad_norm': 0.8468216061592102, 'learning_rate': 3.6363636363636364e-05, 'epoch': 3.7}\n",
      "{'loss': 0.4851, 'grad_norm': 0.7540938854217529, 'learning_rate': 3.272727272727273e-05, 'epoch': 3.78}\n",
      "{'loss': 0.5088, 'grad_norm': 0.6943033337593079, 'learning_rate': 2.909090909090909e-05, 'epoch': 3.85}\n",
      "{'loss': 0.4536, 'grad_norm': 0.6293154954910278, 'learning_rate': 2.5454545454545454e-05, 'epoch': 3.93}\n",
      "{'loss': 0.7124, 'grad_norm': 1.1205803155899048, 'learning_rate': 2.1818181818181818e-05, 'epoch': 4.0}\n",
      "{'loss': 0.4184, 'grad_norm': 0.572246253490448, 'learning_rate': 1.8181818181818182e-05, 'epoch': 4.07}\n",
      "{'loss': 0.4148, 'grad_norm': 0.5867884755134583, 'learning_rate': 1.4545454545454545e-05, 'epoch': 4.15}\n",
      "{'loss': 0.3857, 'grad_norm': 0.6003087162971497, 'learning_rate': 1.0909090909090909e-05, 'epoch': 4.22}\n",
      "{'loss': 0.3693, 'grad_norm': 0.5988588929176331, 'learning_rate': 7.272727272727272e-06, 'epoch': 4.3}\n",
      "{'loss': 0.4228, 'grad_norm': 0.6816331744194031, 'learning_rate': 3.636363636363636e-06, 'epoch': 4.37}\n",
      "{'loss': 0.452, 'grad_norm': 0.6094508171081543, 'learning_rate': 0.0, 'epoch': 4.44}\n",
      "{'train_runtime': 3146.3974, 'train_samples_per_second': 0.153, 'train_steps_per_second': 0.019, 'train_loss': 0.8377200906475385, 'epoch': 4.44}\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "# ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ãƒ¢ãƒ¼ãƒ‰è¨­å®š\n",
    "model = FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# æ¨è«–å®Ÿè¡Œ\n",
    "inference_results = inference(eval_dataset, model, tokenizer)\n",
    "\n",
    "# çµæœã‚’è¡¨ç¤º\n",
    "for i, result in enumerate(inference_results):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"Input: {result['input']}\")\n",
    "    print(f\"Generated Output: {result['generated_output']}\")\n",
    "    print(f\"Expected Output: {result['expected_output']}\")\n",
    "    print(\"----\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
