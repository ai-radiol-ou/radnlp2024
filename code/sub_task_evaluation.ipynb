{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jubuntu/anaconda3/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_micro_f2(data, label, pos_label=1):\n",
    "    \"\"\"\n",
    "    Calculate micro F2 score for a specific label.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame containing true and predicted labels.\n",
    "        label (str): The column name of the label to calculate F2 score.\n",
    "        pos_label (int): The label to treat as positive (default is 1).\n",
    "\n",
    "    Returns:\n",
    "        float: Micro F2 score for the given label.\n",
    "    \"\"\"\n",
    "    return fbeta_score(\n",
    "        data[f\"{label}_true\"],\n",
    "        data[f\"{label}_pred\"],\n",
    "        beta=2.0,\n",
    "        pos_label=pos_label,\n",
    "        average=\"binary\",\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "def evaluate_micro_f2(labels, predictions):\n",
    "    \"\"\"\n",
    "    Evaluate micro F2 scores for all target labels.\n",
    "\n",
    "    Args:\n",
    "        labels (pd.DataFrame): True labels for the dataset.\n",
    "        predictions (pd.DataFrame): Predicted labels for the dataset.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing micro F2 scores for each label and overall score.\n",
    "    \"\"\"\n",
    "    # Merge the true labels and predictions on id and sentence_index\n",
    "    merged = pd.merge(labels, predictions, on=[\"id\", \"sentence_index\"], suffixes=(\"_true\", \"_pred\"))\n",
    "\n",
    "    # Define the target labels\n",
    "    target_labels = [\n",
    "        \"measure\", \"extension\", \"atelectasis\", \"satellite\",\n",
    "        \"lymphadenopathy\", \"pleural\", \"distant\"\n",
    "    ]\n",
    "\n",
    "    # Calculate Overall micro F2.0 (Measure ~ Distant)\n",
    "    overall_true = merged[[f\"{label}_true\" for label in target_labels]].values.ravel()\n",
    "    overall_pred = merged[[f\"{label}_pred\" for label in target_labels]].values.ravel()\n",
    "    overall_f2 = fbeta_score(overall_true, overall_pred, beta=2.0, pos_label=1, average=\"binary\", zero_division=0)\n",
    "\n",
    "    # Calculate Inclusion micro F2.0 (Omittable with pos_label=0)\n",
    "    inclusion_f2 = calculate_micro_f2(merged, \"omittable\", pos_label=0)\n",
    "\n",
    "    # Initialize a dictionary to store results\n",
    "    results = {\"Overall micro F2.0\": overall_f2, \"Inclusion micro F2.0\": inclusion_f2}\n",
    "\n",
    "    # Calculate F2 scores for individual labels\n",
    "    for label in target_labels:\n",
    "        results[f\"{label.capitalize()} micro F2.0\"] = calculate_micro_f2(merged, label)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "# metrics = evaluate_micro_f2(labels, predictions)\n",
    "# print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Overall micro F2.0': 0.7467330429371499,\n",
       " 'Inclusion micro F2.0': 0.8404255319148936,\n",
       " 'Measure micro F2.0': 0.579064587973274,\n",
       " 'Extension micro F2.0': 0.8547008547008546,\n",
       " 'Atelectasis micro F2.0': 0.8602150537634409,\n",
       " 'Satellite micro F2.0': 0.8,\n",
       " 'Lymphadenopathy micro F2.0': 0.8552631578947368,\n",
       " 'Pleural micro F2.0': 0.8590308370044053,\n",
       " 'Distant micro F2.0': 0.6637168141592921}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = pd.read_csv('../radnlp_2024_train_val_20240731/ja/sub_task/val/label.csv')\n",
    "submission = pd.read_csv('../sentence_classifications_gpt-4o-2024-05-13.csv')\n",
    "\n",
    "evaluate_micro_f2(label, submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Overall micro F2.0': 0.8040752351097179,\n",
       " 'Inclusion micro F2.0': 0.9038461538461539,\n",
       " 'Measure micro F2.0': 0.5889281507656066,\n",
       " 'Extension micro F2.0': 0.85667215815486,\n",
       " 'Atelectasis micro F2.0': 0.8895705521472391,\n",
       " 'Satellite micro F2.0': 0.8399999999999999,\n",
       " 'Lymphadenopathy micro F2.0': 0.9595070422535211,\n",
       " 'Pleural micro F2.0': 0.8783783783783784,\n",
       " 'Distant micro F2.0': 0.8541666666666666}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = pd.read_csv('../radnlp_2024_train_val_20240731/ja/sub_task/train/label.csv')\n",
    "submission = pd.read_csv('../sentence_classifications_o1-preview-2024-09-12.csv')\n",
    "\n",
    "evaluate_micro_f2(label, submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最終subへの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_micro_f2(data, label, pos_label=1):\n",
    "    return fbeta_score(\n",
    "        data[f\"{label}_true\"],\n",
    "        data[f\"{label}_pred\"],\n",
    "        beta=2.0,\n",
    "        pos_label=pos_label,\n",
    "        average=\"binary\",\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "def evaluate_micro_f2(labels, predictions):\n",
    "    \"\"\"\n",
    "    Evaluate micro F2 scores for all target labels.\n",
    "    Additionally, return rows where predictions differ from labels.\n",
    "    \"\"\"\n",
    "    # Merge the true labels and predictions on id and sentence_index\n",
    "    merged = pd.merge(labels, predictions, on=[\"id\", \"sentence_index\"], suffixes=(\"_true\", \"_pred\"))\n",
    "\n",
    "    # Define the target labels\n",
    "    target_labels = [\n",
    "        \"measure\", \"extension\", \"atelectasis\", \"satellite\",\n",
    "        \"lymphadenopathy\", \"pleural\", \"distant\"\n",
    "    ]\n",
    "\n",
    "    # Calculate Overall micro F2.0 (Measure ~ Distant)\n",
    "    overall_true = merged[[f\"{label}_true\" for label in target_labels]].values.ravel()\n",
    "    overall_pred = merged[[f\"{label}_pred\" for label in target_labels]].values.ravel()\n",
    "    overall_f2 = fbeta_score(overall_true, overall_pred, beta=2.0, pos_label=1, average=\"binary\", zero_division=0)\n",
    "\n",
    "    # Calculate Inclusion micro F2.0 (Omittable with pos_label=0)\n",
    "    inclusion_f2 = calculate_micro_f2(merged, \"omittable\", pos_label=0)\n",
    "\n",
    "    # Initialize a dictionary to store results\n",
    "    results = {\"Overall micro F2.0\": overall_f2, \"Inclusion micro F2.0\": inclusion_f2}\n",
    "\n",
    "    # Calculate F2 scores for individual labels\n",
    "    for label in target_labels:\n",
    "        results[f\"{label.capitalize()} micro F2.0\"] = calculate_micro_f2(merged, label)\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # ここから「正解と予測が異なる行のみ」を抽出するロジック\n",
    "    # --------------------------------------------------------------\n",
    "    # 1) ターゲットとする列名を作成\n",
    "    target_true_cols = [f\"{lbl}_true\" for lbl in target_labels]\n",
    "    target_pred_cols = [f\"{lbl}_pred\" for lbl in target_labels]\n",
    "    \n",
    "    # 2) 上で定義した全ラベルの true列 と pred列 が 1つでも不一致の行を取り出す\n",
    "    mismatch_mask = (merged[target_true_cols].values != merged[target_pred_cols].values).any(axis=1)\n",
    "    diff_rows = merged[mismatch_mask].copy()\n",
    "\n",
    "    # 必要なら omittable_true / omittable_pred も含める場合は以下のように拡張：\n",
    "    # target_true_cols.append(\"omittable_true\")\n",
    "    # target_pred_cols.append(\"omittable_pred\")\n",
    "    # 再度マスクを取り直す\n",
    "\n",
    "    return results, diff_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, diff = evaluate_micro_f2(label, submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff.to_csv('../model_outputs/o1preview_diff_sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
